{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OCRed OCRed (pronounced as OCR'd ) provides clever, simple, and intuitive wrapper functionalities for OCRing specific text material. You don't want to learn OCR or the libraries that will help you perform OCR , but you need to OCR something? This friendly neighborhood library hides all of that stuff under simple functions like ocr_meaningful_text() . In other words, instead of manual preprocessing, looking for an OCR library, learning the library, then finally getting what you were looking for, use OCRed instead. On the other hand, if you want to learn OCR and use the famous OCR libraries by yourself, then this library is not for you. But, it still can be a good start for your journey! Structure OCR is performed using the OCR class and preprocessing of an image is performed using the Preprocessor class. All the details are available in the documentation . Installation Install Tesseract for your OS and add it to PATH The installation guide is available here Use pip magic OCRed uses modern Python packaging and can be installed using pip - python -m pip install ocred Usage example # OCRing a book import ocred ocr = ocred.OCR( False, # is_scanned -> to preprocess the image \"path/to/an/image\", # path ) ocr.ocr_meaningful_text(save_output=True) # OCRing a signboard import ocred ocr = ocred.OCR( True, # is_scanned -> sign boards don't need to be preprocessed \"path/to/an/image\", # path ) extracted_text = ocr.ocr_sparse_text() print(extracted_text) # OCRing an invoice import ocred ocr = ocred.OCR( True, # is_scanned -> invoices don't need to be preprocessed \"path/to/an/image\", # path ) extracted_text = ocr.ocr_sparse_text() print(extracted_text) extraxted_info = ocr.process_extracted_text_from_invoice() print(extraxted_info) # manually preprocessing an image import cv2 from scipy import ndimage from ocred import Preprocessor preprocessed = Preprocessor(\"path/to/image\") # scan the image and copy the scanned image scanned = preprocessed.scan(inplace=True) orig = scanned.copy() # remove noise noise_free = preprocessed.remove_noise( inplace=True, overriden_image=scanned ) # thicken the ink to draw Hough lines better thickened = preprocessed.thicken_font( inplace=True, overriden_image=noise_free ) # calculate the median angle of all the Hough lines _, median_angle = preprocessed.rotate( inplace=True, overriden_image=thickened ) # rotate the original scanned image rotated = ndimage.rotate(orig, median_angle) # remove noise again final_img = preprocessed.remove_noise(inplace=True, overriden_image=rotated) cv2.imwrite(\"preprocessed.png\", final_img) Testing The tests are present in the tests directory. New tests must be added with any additional features. To run the tests - pytest Some examples \u091c\u092f\u092a\u0941\u0930 JAIPUR 321 \u0906\u0917\u0930\u093e AGRA 554 \u0936\u094d\u0930\u0940 \u0917\u0917\u093e\u0902\u0928\u0917\u0930 242 SRIGANGANAGAR JODHPUR 261 \u091c\u094b\u0927\u092a\u0941\u0930 Preface This book deals with computer architecture as well as computer organization and design. Computer architecture is concerned with the structure and behavior of the various functional modules of the computer and how they interact to provide the processing needs of the user. Computer organization is concerned with the way the hardware components are connected together to form a computer system. Computer design is concerned with the development of the hardware for the computer taking into consideration a given set of specifications. The book provides the basic knowledge necessary to understand the hardware operation of digital computers and covers the three subjects associated with computer hardware. Chapters 1 through 4 present the various digital components used in the organization and design of digital computers. Chapters 5 through 7 show the detailed steps that a designer must go through in order to design an elementary basic computer. Chapters 8 through 10 deal with the organization and architecture of the central processing unit. Chapters 11 and 12 present the organization and architecture of input-output and memory. Chapter 13 introduces the concept of multiprocessing. The plan of the book is to present the simpler material first and introduce the more advanced subjects later, Thus, the first seven chapters cover material needed for the basic understanding of computer organization, design, and programming of a simple digital computer. The last six chapters present the organization and architecture of the separate functional units of the digital computer with an emphasis \u2018on more advanced topics. \u2018The material in the third edition is organized in the same manner as in the second edition and many of the features remain the same. The third edition, however, offers several improvements over the second edition. All chapters \u2018two (6 and 10) have been completely revised to bring the material up to date and to clarify the presentation. Two new chapters were added: chapter 9 on pipeline and vector processing, and chapter 13 on multiprocessors. Two sections deal with the reduced instruction set computer (RISC). Chapter 5 has been revised completely to simplify and clarify the design of the basic computer. New problems have been formulated for eleven of the thirteen chapters. \u2018The physical organization of a particular computer including its registers organisms of our globe, including hydrogen, sodiurn, magnesiuia, and iron. May it not be thai, at least, the brighter stars are like our Sun, the upholding and energizing centres of systems of worlds, adapted to be the abode of living beings? \u2014 William Hugeins, 1865 All my life I have wondered about the possibility of life elsewhere. What would it be like? Of what would it be made? All living things on our planet are constructed of organic molecules ~ complex microscopic architectures in which the carbon atom plays a central role. There was once a time before life, when the Earth was barren and utterly desolate. Our world is now overflowing with life. How did it come about? How, in the absence of life, were carbon-based organic molecules made? How did the first living things arise? How did life evolve to produce beings as elaborate and complex as we, able to explore the mystery of Our Own origins? And on ihe countless other planets that many circle other suns, is there life also? Is extraterrestrial life, if it exists, based on the same organic molecules as life on Earth? Do the beings of other worlds look much like life on Earth? Or are they stunningly different \u2014 other adaptations to other environments? What else is possible? The nature of life on Earth and the search for life elsewhere are two sides of the sarne question \u2014 the search for who we are. In the great dark between the stars there are clouds of gas and dust and organic matter. Dozens of different kinds of organic molecules have been found there by radio telescopes. The abundance of these molecules suggests that the stuff of life is everywhere. Perhaps the origin and evolution of life is, given enough time, a cosmic inevitability. On some of the billions of planets in the Milky Way Galaxy, life may never arise. On others, it May arise and die out, or never evolve beyond its simplest forms. And on some small fraction of worlds there may 35 Contributing If you want to contribute to OCRed (thanks!), please have a look at our Contributing Guide .","title":"Home"},{"location":"#ocred","text":"OCRed (pronounced as OCR'd ) provides clever, simple, and intuitive wrapper functionalities for OCRing specific text material. You don't want to learn OCR or the libraries that will help you perform OCR , but you need to OCR something? This friendly neighborhood library hides all of that stuff under simple functions like ocr_meaningful_text() . In other words, instead of manual preprocessing, looking for an OCR library, learning the library, then finally getting what you were looking for, use OCRed instead. On the other hand, if you want to learn OCR and use the famous OCR libraries by yourself, then this library is not for you. But, it still can be a good start for your journey!","title":"OCRed"},{"location":"#structure","text":"OCR is performed using the OCR class and preprocessing of an image is performed using the Preprocessor class. All the details are available in the documentation .","title":"Structure"},{"location":"#installation","text":"Install Tesseract for your OS and add it to PATH The installation guide is available here Use pip magic OCRed uses modern Python packaging and can be installed using pip - python -m pip install ocred","title":"Installation"},{"location":"#usage-example","text":"# OCRing a book import ocred ocr = ocred.OCR( False, # is_scanned -> to preprocess the image \"path/to/an/image\", # path ) ocr.ocr_meaningful_text(save_output=True) # OCRing a signboard import ocred ocr = ocred.OCR( True, # is_scanned -> sign boards don't need to be preprocessed \"path/to/an/image\", # path ) extracted_text = ocr.ocr_sparse_text() print(extracted_text) # OCRing an invoice import ocred ocr = ocred.OCR( True, # is_scanned -> invoices don't need to be preprocessed \"path/to/an/image\", # path ) extracted_text = ocr.ocr_sparse_text() print(extracted_text) extraxted_info = ocr.process_extracted_text_from_invoice() print(extraxted_info) # manually preprocessing an image import cv2 from scipy import ndimage from ocred import Preprocessor preprocessed = Preprocessor(\"path/to/image\") # scan the image and copy the scanned image scanned = preprocessed.scan(inplace=True) orig = scanned.copy() # remove noise noise_free = preprocessed.remove_noise( inplace=True, overriden_image=scanned ) # thicken the ink to draw Hough lines better thickened = preprocessed.thicken_font( inplace=True, overriden_image=noise_free ) # calculate the median angle of all the Hough lines _, median_angle = preprocessed.rotate( inplace=True, overriden_image=thickened ) # rotate the original scanned image rotated = ndimage.rotate(orig, median_angle) # remove noise again final_img = preprocessed.remove_noise(inplace=True, overriden_image=rotated) cv2.imwrite(\"preprocessed.png\", final_img)","title":"Usage example"},{"location":"#testing","text":"The tests are present in the tests directory. New tests must be added with any additional features. To run the tests - pytest","title":"Testing"},{"location":"#some-examples","text":"\u091c\u092f\u092a\u0941\u0930 JAIPUR 321 \u0906\u0917\u0930\u093e AGRA 554 \u0936\u094d\u0930\u0940 \u0917\u0917\u093e\u0902\u0928\u0917\u0930 242 SRIGANGANAGAR JODHPUR 261 \u091c\u094b\u0927\u092a\u0941\u0930 Preface This book deals with computer architecture as well as computer organization and design. Computer architecture is concerned with the structure and behavior of the various functional modules of the computer and how they interact to provide the processing needs of the user. Computer organization is concerned with the way the hardware components are connected together to form a computer system. Computer design is concerned with the development of the hardware for the computer taking into consideration a given set of specifications. The book provides the basic knowledge necessary to understand the hardware operation of digital computers and covers the three subjects associated with computer hardware. Chapters 1 through 4 present the various digital components used in the organization and design of digital computers. Chapters 5 through 7 show the detailed steps that a designer must go through in order to design an elementary basic computer. Chapters 8 through 10 deal with the organization and architecture of the central processing unit. Chapters 11 and 12 present the organization and architecture of input-output and memory. Chapter 13 introduces the concept of multiprocessing. The plan of the book is to present the simpler material first and introduce the more advanced subjects later, Thus, the first seven chapters cover material needed for the basic understanding of computer organization, design, and programming of a simple digital computer. The last six chapters present the organization and architecture of the separate functional units of the digital computer with an emphasis \u2018on more advanced topics. \u2018The material in the third edition is organized in the same manner as in the second edition and many of the features remain the same. The third edition, however, offers several improvements over the second edition. All chapters \u2018two (6 and 10) have been completely revised to bring the material up to date and to clarify the presentation. Two new chapters were added: chapter 9 on pipeline and vector processing, and chapter 13 on multiprocessors. Two sections deal with the reduced instruction set computer (RISC). Chapter 5 has been revised completely to simplify and clarify the design of the basic computer. New problems have been formulated for eleven of the thirteen chapters. \u2018The physical organization of a particular computer including its registers organisms of our globe, including hydrogen, sodiurn, magnesiuia, and iron. May it not be thai, at least, the brighter stars are like our Sun, the upholding and energizing centres of systems of worlds, adapted to be the abode of living beings? \u2014 William Hugeins, 1865 All my life I have wondered about the possibility of life elsewhere. What would it be like? Of what would it be made? All living things on our planet are constructed of organic molecules ~ complex microscopic architectures in which the carbon atom plays a central role. There was once a time before life, when the Earth was barren and utterly desolate. Our world is now overflowing with life. How did it come about? How, in the absence of life, were carbon-based organic molecules made? How did the first living things arise? How did life evolve to produce beings as elaborate and complex as we, able to explore the mystery of Our Own origins? And on ihe countless other planets that many circle other suns, is there life also? Is extraterrestrial life, if it exists, based on the same organic molecules as life on Earth? Do the beings of other worlds look much like life on Earth? Or are they stunningly different \u2014 other adaptations to other environments? What else is possible? The nature of life on Earth and the search for life elsewhere are two sides of the sarne question \u2014 the search for who we are. In the great dark between the stars there are clouds of gas and dust and organic matter. Dozens of different kinds of organic molecules have been found there by radio telescopes. The abundance of these molecules suggests that the stuff of life is everywhere. Perhaps the origin and evolution of life is, given enough time, a cosmic inevitability. On some of the billions of planets in the Milky Way Galaxy, life may never arise. On others, it May arise and die out, or never evolve beyond its simplest forms. And on some small fraction of worlds there may 35","title":"Some examples"},{"location":"#contributing","text":"If you want to contribute to OCRed (thanks!), please have a look at our Contributing Guide .","title":"Contributing"},{"location":"changelog/","text":"Unreleased Features OCRed can now be built from archive ( #56 ) Breaking changes text_to_speech is deprecated and removed ( #58 ) Maintenance Added a check for docs in the CI ( #59 ) Fixed failing doc deployment ( #59 ) Added pyproject-fmt pre-commit hook ( #57 ) Fixed building from archive (tarballs) ( #56 ) v0.1.2 Maintenance Removed capitalisation from PyPI 's name ( OCRed -> ocred ) ( #52 ) Bug fixes Fixed the DeprecatingWarning in text_to_speech ( #52 ) Removed capitalisation from PyPI 's name ( OCRed -> ocred ) ( #52 ) v0.1.1 Maintenance Updated classifiers and links in pyproject.toml ( #49 ) nltk is now not a part of the default dependencies ( #49 ) Added __version__ to OCRed 's namespace ( #50 ) Deprecations text_to_speech is deprecated and will be removed in v0.2.0 , use gTTS manually ( #50 ) v0.1.0 Added ability to OCR various textual mediums. Added ability to Preprocess images. Infrastructure built with GitHub Actions , hatch , Codecov , and readthedocs . Optimised algorithms with inplace edits. Added documentation with mkdocstrings . Other chore work like pre-commit , nox support, etc. Tests with pytest and coverage with pytest-cov .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"Unreleased"},{"location":"changelog/#features","text":"OCRed can now be built from archive ( #56 )","title":"Features"},{"location":"changelog/#breaking-changes","text":"text_to_speech is deprecated and removed ( #58 )","title":"Breaking changes"},{"location":"changelog/#maintenance","text":"Added a check for docs in the CI ( #59 ) Fixed failing doc deployment ( #59 ) Added pyproject-fmt pre-commit hook ( #57 ) Fixed building from archive (tarballs) ( #56 )","title":"Maintenance"},{"location":"changelog/#v012","text":"","title":"v0.1.2"},{"location":"changelog/#maintenance_1","text":"Removed capitalisation from PyPI 's name ( OCRed -> ocred ) ( #52 )","title":"Maintenance"},{"location":"changelog/#bug-fixes","text":"Fixed the DeprecatingWarning in text_to_speech ( #52 ) Removed capitalisation from PyPI 's name ( OCRed -> ocred ) ( #52 )","title":"Bug fixes"},{"location":"changelog/#v011","text":"","title":"v0.1.1"},{"location":"changelog/#maintenance_2","text":"Updated classifiers and links in pyproject.toml ( #49 ) nltk is now not a part of the default dependencies ( #49 ) Added __version__ to OCRed 's namespace ( #50 )","title":"Maintenance"},{"location":"changelog/#deprecations","text":"text_to_speech is deprecated and will be removed in v0.2.0 , use gTTS manually ( #50 )","title":"Deprecations"},{"location":"changelog/#v010","text":"Added ability to OCR various textual mediums. Added ability to Preprocess images. Infrastructure built with GitHub Actions , hatch , Codecov , and readthedocs . Optimised algorithms with inplace edits. Added documentation with mkdocstrings . Other chore work like pre-commit , nox support, etc. Tests with pytest and coverage with pytest-cov .","title":"v0.1.0"},{"location":"conduct/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at saransh0701@gmail.com . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"conduct/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"conduct/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"conduct/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"conduct/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at saransh0701@gmail.com . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"conduct/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"conduct/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"conduct/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"conduct/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"conduct/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"contributing/","text":"Contributing guide If you are planning to develop OCRed , or want to use the latest commit of OCRed on your local machine, you might want to install it from the source. This installation is not recommended for users who want to use the stable version of OCRed . The steps below describe the installation process of OCRed 's latest commit. It also describes how to test OCRed 's codebase and build OCRed 's documentation. Note : OCRed uses Scikit-HEP's developer information as a reference for all the development work. The guide is a general and much more explained collection of documentation available for developing Scikit-HEP packages. OCRed is not a Scikit-HEP package, but it still loosely follows this developer guide as it is absolutely amazing! Installing OCRed We recommend using a virtual environment to install OCRed . This would isolate the library from your global Python environment, which would be beneficial for reproducing bugs, and the overall development of OCRed . The first step would be to clone OCRed - git clone https://github.com/Scikit-hep/OCRed.git and then we can change the current working directory and enter OCRed - cd OCRed Creating a virtual environment A virtual environment can be set up and activated using venv in both UNIX and Windows systems. UNIX : python3 -m venv .env . .env/bin/activate Windows : python -m venv .env .env\\bin\\activate Installation The developer installation of OCRed comes with a lot of options - test : the test dependencies docs : extra dependencies to build and develop OCRed 's documentation dev : installs the test and docs dependencies nltk : installs nltk These options can be used with pip with the editable ( -e ) mode of installation in the following ways - pip install -e .[dev,test] For example, if you want to install the docs dependencies along with the dependencies included above, use - pip install -e .[dev,test,docs] Adding OCRed for notebooks OCRed can be added to the notebooks using the following commands - python -m ipykernel install --user --name ocred Activating pre-commit OCRed uses a set of pre-commit hooks and the pre-commit bot to format, type-check, and prettify the codebase. The hooks can be installed locally using - pre-commit install This would run the checks every time a commit is created locally. The checks will only run on the files modified by that commit, but the checks can be triggered for all the files using - pre-commit run --all-files If you would like to skip the failing checks and push the code for further discussion, use the --no-verify option with git commit . Testing OCRed OCRed is tested with pytest and xdoctest . pytest is responsible for testing the code, whose configuration is available in pyproject.toml , and on the other hand, xdoctest is responsible for testing the examples available in every docstring, which prevents them from going stale. Additionally, OCRed also uses pytest-cov to calculate the coverage of these unit tests. Running tests locally The tests can be executed using the test dependencies of OCRed in the following way - python -m pytest -ra Running tests with coverage locally The coverage value can be obtained while running the tests using pytest-cov in the following way - python -m pytest -ra --cov=ocred tests/ Running doctests The doctests can be executed using the test dependencies of OCRed in the following way - xdoctest ./src/ocred/ A much more detailed guide on testing with pytest is available here . Documenting OCRed OCRed 's documentation is mainly written in the form of docstrings and Markdown . The docstrings include the description, arguments, examples, return values, and attributes of a class or a function, and the .md files enable us to render this documentation on OCRed 's documentation website. OCRed primarily uses MkDocs and mkdocstrings for rendering documentation on its website. The configuration file ( mkdocs.yml ) for MkDocs can be found here . The documentation is deployed on https://readthedocs.io here . Ideally, with the addition of every new feature to OCRed , documentation should be added using comments, docstrings, and .md files. Building documentation locally The documentation is located in the docs folder of the main repository. This documentation can be generated using the docs dependencies of OCRed in the following way - mkdocs serve The commands executed above will clean any existing documentation build, create a new build (in ./site/ ), and serve it on your localhost . To just build the documentation, use - mkdocs build Nox OCRed supports running various critical commands using nox to make them less intimidating for new developers. All of these commands (or sessions in the language of nox ) - lint , tests , doctests , docs , and build - are defined in noxfile.py . nox can be installed via pip using - pip install nox The default sessions ( lint , tests , and doctests ) can be executed using - nox Running pre-commit with nox The pre-commit hooks can be run with nox in the following way - nox -s lint Running tests with nox Tests can be run with nox in the following way - nox -s tests Building documentation with nox Docs can be built with nox in the following way - nox -s docs Use the following command if you want to deploy the docs on localhost - nox -s docs -- serve","title":"Contributing"},{"location":"contributing/#contributing-guide","text":"If you are planning to develop OCRed , or want to use the latest commit of OCRed on your local machine, you might want to install it from the source. This installation is not recommended for users who want to use the stable version of OCRed . The steps below describe the installation process of OCRed 's latest commit. It also describes how to test OCRed 's codebase and build OCRed 's documentation. Note : OCRed uses Scikit-HEP's developer information as a reference for all the development work. The guide is a general and much more explained collection of documentation available for developing Scikit-HEP packages. OCRed is not a Scikit-HEP package, but it still loosely follows this developer guide as it is absolutely amazing!","title":"Contributing guide"},{"location":"contributing/#installing-ocred","text":"We recommend using a virtual environment to install OCRed . This would isolate the library from your global Python environment, which would be beneficial for reproducing bugs, and the overall development of OCRed . The first step would be to clone OCRed - git clone https://github.com/Scikit-hep/OCRed.git and then we can change the current working directory and enter OCRed - cd OCRed","title":"Installing OCRed"},{"location":"contributing/#creating-a-virtual-environment","text":"A virtual environment can be set up and activated using venv in both UNIX and Windows systems. UNIX : python3 -m venv .env . .env/bin/activate Windows : python -m venv .env .env\\bin\\activate","title":"Creating a virtual environment"},{"location":"contributing/#installation","text":"The developer installation of OCRed comes with a lot of options - test : the test dependencies docs : extra dependencies to build and develop OCRed 's documentation dev : installs the test and docs dependencies nltk : installs nltk These options can be used with pip with the editable ( -e ) mode of installation in the following ways - pip install -e .[dev,test] For example, if you want to install the docs dependencies along with the dependencies included above, use - pip install -e .[dev,test,docs]","title":"Installation"},{"location":"contributing/#adding-ocred-for-notebooks","text":"OCRed can be added to the notebooks using the following commands - python -m ipykernel install --user --name ocred","title":"Adding OCRed for notebooks"},{"location":"contributing/#activating-pre-commit","text":"OCRed uses a set of pre-commit hooks and the pre-commit bot to format, type-check, and prettify the codebase. The hooks can be installed locally using - pre-commit install This would run the checks every time a commit is created locally. The checks will only run on the files modified by that commit, but the checks can be triggered for all the files using - pre-commit run --all-files If you would like to skip the failing checks and push the code for further discussion, use the --no-verify option with git commit .","title":"Activating pre-commit"},{"location":"contributing/#testing-ocred","text":"OCRed is tested with pytest and xdoctest . pytest is responsible for testing the code, whose configuration is available in pyproject.toml , and on the other hand, xdoctest is responsible for testing the examples available in every docstring, which prevents them from going stale. Additionally, OCRed also uses pytest-cov to calculate the coverage of these unit tests.","title":"Testing OCRed"},{"location":"contributing/#running-tests-locally","text":"The tests can be executed using the test dependencies of OCRed in the following way - python -m pytest -ra","title":"Running tests locally"},{"location":"contributing/#running-tests-with-coverage-locally","text":"The coverage value can be obtained while running the tests using pytest-cov in the following way - python -m pytest -ra --cov=ocred tests/","title":"Running tests with coverage locally"},{"location":"contributing/#running-doctests","text":"The doctests can be executed using the test dependencies of OCRed in the following way - xdoctest ./src/ocred/ A much more detailed guide on testing with pytest is available here .","title":"Running doctests"},{"location":"contributing/#documenting-ocred","text":"OCRed 's documentation is mainly written in the form of docstrings and Markdown . The docstrings include the description, arguments, examples, return values, and attributes of a class or a function, and the .md files enable us to render this documentation on OCRed 's documentation website. OCRed primarily uses MkDocs and mkdocstrings for rendering documentation on its website. The configuration file ( mkdocs.yml ) for MkDocs can be found here . The documentation is deployed on https://readthedocs.io here . Ideally, with the addition of every new feature to OCRed , documentation should be added using comments, docstrings, and .md files.","title":"Documenting OCRed"},{"location":"contributing/#building-documentation-locally","text":"The documentation is located in the docs folder of the main repository. This documentation can be generated using the docs dependencies of OCRed in the following way - mkdocs serve The commands executed above will clean any existing documentation build, create a new build (in ./site/ ), and serve it on your localhost . To just build the documentation, use - mkdocs build","title":"Building documentation locally"},{"location":"contributing/#nox","text":"OCRed supports running various critical commands using nox to make them less intimidating for new developers. All of these commands (or sessions in the language of nox ) - lint , tests , doctests , docs , and build - are defined in noxfile.py . nox can be installed via pip using - pip install nox The default sessions ( lint , tests , and doctests ) can be executed using - nox","title":"Nox"},{"location":"contributing/#running-pre-commit-with-nox","text":"The pre-commit hooks can be run with nox in the following way - nox -s lint","title":"Running pre-commit with nox"},{"location":"contributing/#running-tests-with-nox","text":"Tests can be run with nox in the following way - nox -s tests","title":"Running tests with nox"},{"location":"contributing/#building-documentation-with-nox","text":"Docs can be built with nox in the following way - nox -s docs Use the following command if you want to deploy the docs on localhost - nox -s docs -- serve","title":"Building documentation with nox"},{"location":"install/","text":"Installation Follow the steps below to install ocred locally. Create a virtual environment Create and activate a virtual environment python -m venv env . env/bin/activate Install OCRed Install Tesseract for your OS and add it to PATH The installation guide is available here pip magic OCRed uses modern Python packaging and can be installed using pip - python -m pip install ocred Build OCRed from source If you want to develop OCRed , or use its latest commit (!can be unstable!), you might want to install it from the source - Install Tesseract for your OS and add it to PATH The installation guide is available here Clone this repository git clone https://github.com/Saransh-cpp/OCRed Change directory cd OCRed Install the package in editable mode with the \"dev\" dependencies python -m pip install -e \".[dev]\" Feel free to read our Contributing Guide for more information on developing OCRed .","title":"Installation"},{"location":"install/#installation","text":"Follow the steps below to install ocred locally.","title":"Installation"},{"location":"install/#create-a-virtual-environment","text":"Create and activate a virtual environment python -m venv env . env/bin/activate","title":"Create a virtual environment"},{"location":"install/#install-ocred","text":"Install Tesseract for your OS and add it to PATH The installation guide is available here pip magic OCRed uses modern Python packaging and can be installed using pip - python -m pip install ocred","title":"Install OCRed"},{"location":"install/#build-ocred-from-source","text":"If you want to develop OCRed , or use its latest commit (!can be unstable!), you might want to install it from the source - Install Tesseract for your OS and add it to PATH The installation guide is available here Clone this repository git clone https://github.com/Saransh-cpp/OCRed Change directory cd OCRed Install the package in editable mode with the \"dev\" dependencies python -m pip install -e \".[dev]\" Feel free to read our Contributing Guide for more information on developing OCRed .","title":"Build OCRed from source"},{"location":"reference/","text":"OCR class Performs OCR on a given image, saves an image with boxes around the words, and converts the extracted text to an MP3 file. Add Tesseract OCR's installation location in PATH for functions using it to work. Parameters: Name Type Description Default preprocess bool Set True if the image is a real life photo of some large meaningful (page of a book). Usually set to False when OCRing using ocr_meaningful_text to preprocess the image. Set False if the image is a scanned photo (an e-book). It will not be pre-processed before OCRing. All the preprocessing is performed inplace to maintain efficiency. Use the Preprocessor class manually to have more control! required path str Path of the image to be used. required Examples: >>> import sys >>> sys . displayhook = lambda x : None >>> import ocred >>> ocr = ocred . OCR ( ... False , # is_scanned -> to preprocess the image ... \"./images/Page.png\" ... ) >>> ocr . ocr_meaningful_text ( save_output = True ) Source code in ocred\\ocr.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 class OCR : \"\"\" Performs OCR on a given image, saves an image with boxes around the words, and converts the extracted text to an MP3 file. Add Tesseract OCR's installation location in PATH for functions using it to work. Args: preprocess: Set True if the image is a real life photo of some large meaningful (page of a book). Usually set to False when OCRing using `ocr_meaningful_text` to preprocess the image. Set False if the image is a scanned photo (an e-book). It will not be pre-processed before OCRing. All the preprocessing is performed inplace to maintain efficiency. Use the `Preprocessor` class manually to have more control! path: Path of the image to be used. Examples: >>> import sys >>> sys.displayhook = lambda x: None >>> import ocred >>> ocr = ocred.OCR( ... False, # is_scanned -> to preprocess the image ... \"./images/Page.png\" ... ) >>> ocr.ocr_meaningful_text(save_output=True) \"\"\" def __init__ ( self , preprocess : bool , path : str ) -> None : self . path = path self . preprocess = preprocess if self . preprocess : preprocessed = Preprocessor ( self . path ) # scan the image and copy the scanned image scanned = preprocessed . scan ( inplace = True ) orig = scanned . copy () # remove noise noise_free = preprocessed . remove_noise ( inplace = True , overriden_image = scanned ) # thicken the ink to draw Hough lines better thickened = preprocessed . thicken_font ( inplace = True , overriden_image = noise_free ) # calculate the median angle of all the Hough lines _ , median_angle = preprocessed . rotate ( inplace = True , overriden_image = thickened ) # rotate the original scanned image rotated = ndimage . rotate ( orig , median_angle ) # remove noise again final_img = preprocessed . remove_noise ( inplace = True , overriden_image = rotated ) cv2 . imwrite ( \"preprocessed.png\" , final_img ) self . path = \"preprocessed.png\" def ocr_meaningful_text ( self , * , save_output : Optional [ bool ] = False ) -> str : \"\"\" Performs OCR on long meaningful text documents and saves the image with boxes around the words. For example - books, PDFs etc. Args: save_output: Saves the text to `output.txt` file. Returns: text: The extracted text. \"\"\" # reading the image img = cv2 . imread ( self . path ) # extracting the text self . oriented_text = pytesseract . image_to_string ( img , config = \"-l eng --oem 1\" ) self . text = self . oriented_text . replace ( \"- \\n \" , \"\" ) . replace ( \" \\n \" , \" \" ) # adding boxes around the words boxes = pytesseract . image_to_data ( img ) for z , box in enumerate ( boxes . splitlines ()): if z != 0 : box = box . split () # if the data has a word if len ( box ) == 12 : x , y = int ( box [ 6 ]), int ( box [ 7 ]) h , w = int ( box [ 8 ]), int ( box [ 9 ]) cv2 . rectangle ( img , ( x , y ), ( x + h , y + w ), ( 0 , 0 , 255 ), 1 ) cv2 . imwrite ( \"OCR.png\" , img ) if save_output : self . save_output () return self . text def ocr_sparse_text ( self , * , languages : Optional [ List [ str ]] = [ \"en\" , \"hi\" ], decoder : Optional [ str ] = \"greedy\" , save_output : Optional [ bool ] = False , ) -> str : \"\"\" Performs OCR on sparse text and saves the image with boxes around the words. This method can be used to OCR documents in which the characters don't form any proper/meaningful sentences, or if there are very less meaningful sentences, for example - bills, sign-boards etc. Args: languages: A list of languages that the signboard possible has. Note: Provide only the languages that are present in the image, adding additional languages misguides the model. decoder: If the document has a larger number of meaningful sentences then use \"beamsearch\". For most of the cases \"greedy\" works very well. save_output: Saves the text to `output.txt` file. Returns: text: The extracted text. \"\"\" self . text = \"\" # reading the image using open-cv and easyocr img = cv2 . imread ( self . path ) reader = easyocr . Reader ( languages ) # slow for the first time (also depends upon CPU/GPU) self . detailed_text = reader . readtext ( self . path , decoder = decoder , batch_size = 5 ) for text in self . detailed_text : # extracting the coordinates to highlight the text coords_lower = text [ 0 ][: 2 ] coords_upper = text [ 0 ][ 2 : 4 ] coords_lower . sort ( key = lambda x : x [ 0 ]) pt1 = [ int ( x ) for x in coords_upper [ - 1 ]] coords_lower . sort ( key = lambda x : x [ 0 ]) pt2 = [ int ( x ) for x in coords_lower [ - 1 ]] # highlighting the text cv2 . rectangle ( img , pt1 , pt2 , ( 0 , 0 , 255 ), 1 ) self . text = self . text + \" \" + text [ - 2 ] cv2 . imwrite ( \"OCR.png\" , img ) if save_output : self . save_output () return self . text def process_extracted_text_from_invoice ( self ) -> Dict [ str , Any ]: \"\"\" This method processes the extracted text from invoices, and returns some useful information. Returns: extracted_info: The extracted information. \"\"\" import nltk nltk . download ( \"punkt\" ) nltk . download ( \"wordnet\" ) nltk . download ( \"stopwords\" ) self . extracted_info = {} self . text_list = self . text . split ( \" \" ) # find date date_re = re . compile ( r \"^([1-9]|0[1-9]|1[0-9]|2[0-9]|3[0-1])(\\.|-|\\/)([1-9]|0[1-9]|1[0-2])(\\.|-|\\/)([0-9][0-9]|19[0-9][0-9]|20[0-9][0-9])$\" , ) date = list ( filter ( date_re . match , self . text_list )) # find phone number phone_number_re = re . compile ( r \"((\\+*)((0[ -]*)*|((91 )*))((\\d {12} )+|(\\d {10} )+))|\\d {5} ([- ]*)\\d {6} \" , ) phone_number = list ( filter ( phone_number_re . match , self . text_list )) # find place place = self . detailed_text [ 0 ][ - 2 ] # remove puntuations and redundant words tokenizer = nltk . RegexpTokenizer ( r \"\\w+\" ) removed_punctuation = tokenizer . tokenize ( self . text ) stop_words = set ( nltk . corpus . stopwords . words ( \"english\" )) post_processed_word_list = [ w for w in removed_punctuation if w not in stop_words ] # find order number order_number : Union [ str , int ] = \"\" for i in range ( len ( post_processed_word_list )): if post_processed_word_list [ i ] . lower () == \"order\" : try : order_number = int ( post_processed_word_list [ i + 1 ]) except Exception : order_number = post_processed_word_list [ i + 2 ] break # find total price price : Union [ List [ Any ], str ] = \"\" # try finding a number with Rs, INR, \u20b9 or \u0930\u0947 in front of it or Rs, INR at the end # of it try : price = re . findall ( r \"(?:Rs\\.?|INR|\u20b9\\.?|\u0930\u0947\\.?)\\s*(\\d+(?:[.,]\\d+)*)|(\\d+(?:[.,]\\d+)*)\\s*(?:Rs\\.?|INR)\" , self . text , ) price = list ( map ( float , price )) price = max ( price ) # try finding numbers with \"grand total\" or \"total\" written in front of them except ValueError : lowered_list = [ x . lower () for x in post_processed_word_list ] if \"grand\" in lowered_list : indices = [ i for i , x in enumerate ( lowered_list ) if x == \"grand\" ] i = indices [ - 1 ] price = post_processed_word_list [ i + 2 ] elif \"total\" in lowered_list : indices = [ i for i , x in enumerate ( lowered_list ) if x == \"total\" ] i = indices [ - 1 ] price = post_processed_word_list [ i + 1 ] self . extracted_info . update ( { \"price\" : price , \"date\" : date , \"place\" : place , \"order_number\" : order_number , \"phone_number\" : phone_number , \"post_processed_word_list\" : post_processed_word_list , } ) return self . extracted_info def save_output ( self ) -> None : \"\"\"Saves the extracted text in the `output.txt` file.\"\"\" f = open ( \"output.txt\" , \"w\" , encoding = \"utf-8\" ) f . write ( self . text ) f . close () def text_to_speech ( self , * , lang : Optional [ str ] = \"en\" ) -> None : \"\"\" DANGER: Deprecated since version v0.2.0. Instead, use gTTS manually. Converts the extracted text to speech and save it as an MP3 file. Args: lang: Language of the processed text. \"\"\" raise DeprecationWarning ( \"text_to_speech is deprecated and was removed in v0.2.0; use gTTS manually\" , ) ocr_meaningful_text ( * , save_output = False ) Performs OCR on long meaningful text documents and saves the image with boxes around the words. For example - books, PDFs etc. Parameters: Name Type Description Default save_output Optional [ bool ] Saves the text to output.txt file. False Returns: Name Type Description text str The extracted text. Source code in ocred\\ocr.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def ocr_meaningful_text ( self , * , save_output : Optional [ bool ] = False ) -> str : \"\"\" Performs OCR on long meaningful text documents and saves the image with boxes around the words. For example - books, PDFs etc. Args: save_output: Saves the text to `output.txt` file. Returns: text: The extracted text. \"\"\" # reading the image img = cv2 . imread ( self . path ) # extracting the text self . oriented_text = pytesseract . image_to_string ( img , config = \"-l eng --oem 1\" ) self . text = self . oriented_text . replace ( \"- \\n \" , \"\" ) . replace ( \" \\n \" , \" \" ) # adding boxes around the words boxes = pytesseract . image_to_data ( img ) for z , box in enumerate ( boxes . splitlines ()): if z != 0 : box = box . split () # if the data has a word if len ( box ) == 12 : x , y = int ( box [ 6 ]), int ( box [ 7 ]) h , w = int ( box [ 8 ]), int ( box [ 9 ]) cv2 . rectangle ( img , ( x , y ), ( x + h , y + w ), ( 0 , 0 , 255 ), 1 ) cv2 . imwrite ( \"OCR.png\" , img ) if save_output : self . save_output () return self . text ocr_sparse_text ( * , languages = [ 'en' , 'hi' ], decoder = 'greedy' , save_output = False ) Performs OCR on sparse text and saves the image with boxes around the words. This method can be used to OCR documents in which the characters don't form any proper/meaningful sentences, or if there are very less meaningful sentences, for example - bills, sign-boards etc. Parameters: Name Type Description Default languages Optional [ List [ str ]] A list of languages that the signboard possible has. Note: Provide only the languages that are present in the image, adding additional languages misguides the model. ['en', 'hi'] decoder Optional [ str ] If the document has a larger number of meaningful sentences then use \"beamsearch\". For most of the cases \"greedy\" works very well. 'greedy' save_output Optional [ bool ] Saves the text to output.txt file. False Returns: Name Type Description text str The extracted text. Source code in ocred\\ocr.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def ocr_sparse_text ( self , * , languages : Optional [ List [ str ]] = [ \"en\" , \"hi\" ], decoder : Optional [ str ] = \"greedy\" , save_output : Optional [ bool ] = False , ) -> str : \"\"\" Performs OCR on sparse text and saves the image with boxes around the words. This method can be used to OCR documents in which the characters don't form any proper/meaningful sentences, or if there are very less meaningful sentences, for example - bills, sign-boards etc. Args: languages: A list of languages that the signboard possible has. Note: Provide only the languages that are present in the image, adding additional languages misguides the model. decoder: If the document has a larger number of meaningful sentences then use \"beamsearch\". For most of the cases \"greedy\" works very well. save_output: Saves the text to `output.txt` file. Returns: text: The extracted text. \"\"\" self . text = \"\" # reading the image using open-cv and easyocr img = cv2 . imread ( self . path ) reader = easyocr . Reader ( languages ) # slow for the first time (also depends upon CPU/GPU) self . detailed_text = reader . readtext ( self . path , decoder = decoder , batch_size = 5 ) for text in self . detailed_text : # extracting the coordinates to highlight the text coords_lower = text [ 0 ][: 2 ] coords_upper = text [ 0 ][ 2 : 4 ] coords_lower . sort ( key = lambda x : x [ 0 ]) pt1 = [ int ( x ) for x in coords_upper [ - 1 ]] coords_lower . sort ( key = lambda x : x [ 0 ]) pt2 = [ int ( x ) for x in coords_lower [ - 1 ]] # highlighting the text cv2 . rectangle ( img , pt1 , pt2 , ( 0 , 0 , 255 ), 1 ) self . text = self . text + \" \" + text [ - 2 ] cv2 . imwrite ( \"OCR.png\" , img ) if save_output : self . save_output () return self . text process_extracted_text_from_invoice () This method processes the extracted text from invoices, and returns some useful information. Returns: Name Type Description extracted_info Dict [ str , Any ] The extracted information. Source code in ocred\\ocr.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def process_extracted_text_from_invoice ( self ) -> Dict [ str , Any ]: \"\"\" This method processes the extracted text from invoices, and returns some useful information. Returns: extracted_info: The extracted information. \"\"\" import nltk nltk . download ( \"punkt\" ) nltk . download ( \"wordnet\" ) nltk . download ( \"stopwords\" ) self . extracted_info = {} self . text_list = self . text . split ( \" \" ) # find date date_re = re . compile ( r \"^([1-9]|0[1-9]|1[0-9]|2[0-9]|3[0-1])(\\.|-|\\/)([1-9]|0[1-9]|1[0-2])(\\.|-|\\/)([0-9][0-9]|19[0-9][0-9]|20[0-9][0-9])$\" , ) date = list ( filter ( date_re . match , self . text_list )) # find phone number phone_number_re = re . compile ( r \"((\\+*)((0[ -]*)*|((91 )*))((\\d {12} )+|(\\d {10} )+))|\\d {5} ([- ]*)\\d {6} \" , ) phone_number = list ( filter ( phone_number_re . match , self . text_list )) # find place place = self . detailed_text [ 0 ][ - 2 ] # remove puntuations and redundant words tokenizer = nltk . RegexpTokenizer ( r \"\\w+\" ) removed_punctuation = tokenizer . tokenize ( self . text ) stop_words = set ( nltk . corpus . stopwords . words ( \"english\" )) post_processed_word_list = [ w for w in removed_punctuation if w not in stop_words ] # find order number order_number : Union [ str , int ] = \"\" for i in range ( len ( post_processed_word_list )): if post_processed_word_list [ i ] . lower () == \"order\" : try : order_number = int ( post_processed_word_list [ i + 1 ]) except Exception : order_number = post_processed_word_list [ i + 2 ] break # find total price price : Union [ List [ Any ], str ] = \"\" # try finding a number with Rs, INR, \u20b9 or \u0930\u0947 in front of it or Rs, INR at the end # of it try : price = re . findall ( r \"(?:Rs\\.?|INR|\u20b9\\.?|\u0930\u0947\\.?)\\s*(\\d+(?:[.,]\\d+)*)|(\\d+(?:[.,]\\d+)*)\\s*(?:Rs\\.?|INR)\" , self . text , ) price = list ( map ( float , price )) price = max ( price ) # try finding numbers with \"grand total\" or \"total\" written in front of them except ValueError : lowered_list = [ x . lower () for x in post_processed_word_list ] if \"grand\" in lowered_list : indices = [ i for i , x in enumerate ( lowered_list ) if x == \"grand\" ] i = indices [ - 1 ] price = post_processed_word_list [ i + 2 ] elif \"total\" in lowered_list : indices = [ i for i , x in enumerate ( lowered_list ) if x == \"total\" ] i = indices [ - 1 ] price = post_processed_word_list [ i + 1 ] self . extracted_info . update ( { \"price\" : price , \"date\" : date , \"place\" : place , \"order_number\" : order_number , \"phone_number\" : phone_number , \"post_processed_word_list\" : post_processed_word_list , } ) return self . extracted_info save_output () Saves the extracted text in the output.txt file. Source code in ocred\\ocr.py 270 271 272 273 274 def save_output ( self ) -> None : \"\"\"Saves the extracted text in the `output.txt` file.\"\"\" f = open ( \"output.txt\" , \"w\" , encoding = \"utf-8\" ) f . write ( self . text ) f . close () text_to_speech ( * , lang = 'en' ) Danger Deprecated since version v0.2.0. Instead, use gTTS manually. Converts the extracted text to speech and save it as an MP3 file. Parameters: Name Type Description Default lang Optional [ str ] Language of the processed text. 'en' Source code in ocred\\ocr.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def text_to_speech ( self , * , lang : Optional [ str ] = \"en\" ) -> None : \"\"\" DANGER: Deprecated since version v0.2.0. Instead, use gTTS manually. Converts the extracted text to speech and save it as an MP3 file. Args: lang: Language of the processed text. \"\"\" raise DeprecationWarning ( \"text_to_speech is deprecated and was removed in v0.2.0; use gTTS manually\" , ) Preprocessor class Preprocesses an image and makes it ready for OCR. Parameters: Name Type Description Default image Union [ str , Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]] Path of the image or a numpy array. required Examples: >>> import cv2 >>> from scipy import ndimage >>> from ocred import Preprocessor >>> # scan the image and copy the scanned image >>> preprocessed = Preprocessor ( \"images/CosmosTwo.jpg\" ) >>> # scan the image and copy the scanned image >>> scanned = preprocessed . scan ( inplace = True ) >>> orig = scanned . copy () >>> # remove noise ... noise_free = preprocessed . remove_noise ( ... inplace = True , overriden_image = scanned ... ) >>> # thicken the ink to draw Hough lines better >>> thickened = preprocessed . thicken_font ( ... inplace = True , overriden_image = noise_free ... ) >>> # calculate the median angle of all the Hough lines >>> _ , median_angle = preprocessed . rotate ( ... inplace = True , overriden_image = thickened ... ) >>> # rotate the original scanned image >>> rotated = ndimage . rotate ( orig , median_angle ) >>> # remove noise again >>> final_img = preprocessed . remove_noise ( inplace = True , overriden_image = rotated ) >>> cv2 . imwrite ( \"preprocessed.png\" , final_img ) True Source code in ocred\\preprocessing.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class Preprocessor : \"\"\" Preprocesses an image and makes it ready for OCR. Args: image: Path of the image or a numpy array. Examples: >>> import cv2 >>> from scipy import ndimage >>> from ocred import Preprocessor >>> # scan the image and copy the scanned image >>> preprocessed = Preprocessor(\"images/CosmosTwo.jpg\") >>> # scan the image and copy the scanned image >>> scanned = preprocessed.scan(inplace=True) >>> orig = scanned.copy() >>> # remove noise ... noise_free = preprocessed.remove_noise( ... inplace=True, overriden_image=scanned ... ) >>> # thicken the ink to draw Hough lines better >>> thickened = preprocessed.thicken_font( ... inplace=True, overriden_image=noise_free ... ) >>> # calculate the median angle of all the Hough lines >>> _, median_angle = preprocessed.rotate( ... inplace=True, overriden_image=thickened ... ) >>> # rotate the original scanned image >>> rotated = ndimage.rotate(orig, median_angle) >>> # remove noise again >>> final_img = preprocessed.remove_noise(inplace=True, overriden_image=rotated) >>> cv2.imwrite(\"preprocessed.png\", final_img) True \"\"\" def __init__ ( self , image : Union [ str , Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]], ) -> None : if isinstance ( image , str ): self . img = cv2 . imread ( image ) else : self . img = image def remove_noise ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , iterations : Optional [ int ] = 1 , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Removes noise from an image. Args: save: Saves the resultant image. inplace: Edits the image inplace. iterations: Number of times the image is processed. overriden_image: Overrides the image passes through the constructor. Returns: noise_free_image: The noise free image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image kernel : Union [ npt . NDArray [ np . int64 ]] = np . ones (( 1 , 1 ), np . uint8 ) img = cv2 . dilate ( img , kernel , iterations = iterations ) kernel = np . ones (( 1 , 1 ), np . uint8 ) img = cv2 . erode ( img , kernel , iterations = iterations ) img = cv2 . morphologyEx ( img , cv2 . MORPH_CLOSE , kernel ) img = cv2 . medianBlur ( img , 3 ) if save : cv2 . imwrite ( \"noise_free.png\" , img ) return self . img def thicken_font ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , iterations : Optional [ int ] = 2 , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Thickens the ink of an image. Args: save: Saves the resultant image. inplace: Edits the image inplace. iterations: Number of times the image is processed. overriden_image: Overrides the image passes through the constructor. Returns: thickened_image: The thickened image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img = cv2 . bitwise_not ( img ) kernel : Union [ npt . NDArray [ np . int64 ]] = np . ones (( 2 , 2 ), np . uint8 ) img = cv2 . dilate ( img , kernel , iterations = iterations ) img = cv2 . bitwise_not ( img ) if save : cv2 . imwrite ( \"thick_font.png\" , img ) return img def scan ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Transforms an image/document view into B&W view (proper scanned colour scheme). Args: save: Saves the resultant image. inplace: Edits the image inplace. overriden_image: Overrides the image passes through the constructor. Returns: scanned_image: The scanned image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) thr = threshold_local ( img , 11 , offset = 10 , method = \"gaussian\" ) img = ( img > thr ) . astype ( \"uint8\" ) * 255 if save : cv2 . imwrite ( \"scanned.png\" , img ) return img def rotate ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Tuple [ Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]], float ]: \"\"\" Rotates an image for a face-on view (view from the top). Args: save: Saves the resultant image. inplace: Edits the image inplace. overriden_image: Overrides the image passes through the constructor. Returns: rotated_image: The rotated image. median_angle: The angly by which it is rotated. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img_edges = cv2 . Canny ( img , 100 , 100 , apertureSize = 3 ) lines = cv2 . HoughLinesP ( img_edges , rho = 1 , theta = np . pi / 180.0 , threshold = 160 , minLineLength = 100 , maxLineGap = 10 , ) angles = [] for [[ x1 , y1 , x2 , y2 ]] in lines : angle = math . degrees ( math . atan2 ( y2 - y1 , x2 - x1 )) angles . append ( angle ) median_angle = float ( np . median ( angles )) img = ndimage . rotate ( img , median_angle ) if save : cv2 . imwrite ( \"rotated.png\" , img ) return img , median_angle remove_noise ( * , save = False , inplace = False , iterations = 1 , overriden_image = None ) Removes noise from an image. Parameters: Name Type Description Default save Optional [ bool ] Saves the resultant image. False inplace Optional [ bool ] Edits the image inplace. False iterations Optional [ int ] Number of times the image is processed. 1 overriden_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None] Overrides the image passes through the constructor. None Returns: Name Type Description noise_free_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]] The noise free image. Source code in ocred\\preprocessing.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def remove_noise ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , iterations : Optional [ int ] = 1 , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Removes noise from an image. Args: save: Saves the resultant image. inplace: Edits the image inplace. iterations: Number of times the image is processed. overriden_image: Overrides the image passes through the constructor. Returns: noise_free_image: The noise free image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image kernel : Union [ npt . NDArray [ np . int64 ]] = np . ones (( 1 , 1 ), np . uint8 ) img = cv2 . dilate ( img , kernel , iterations = iterations ) kernel = np . ones (( 1 , 1 ), np . uint8 ) img = cv2 . erode ( img , kernel , iterations = iterations ) img = cv2 . morphologyEx ( img , cv2 . MORPH_CLOSE , kernel ) img = cv2 . medianBlur ( img , 3 ) if save : cv2 . imwrite ( \"noise_free.png\" , img ) return self . img rotate ( * , save = False , inplace = False , overriden_image = None ) Rotates an image for a face-on view (view from the top). Parameters: Name Type Description Default save Optional [ bool ] Saves the resultant image. False inplace Optional [ bool ] Edits the image inplace. False overriden_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None] Overrides the image passes through the constructor. None Returns: Name Type Description rotated_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]] The rotated image. median_angle float The angly by which it is rotated. Source code in ocred\\preprocessing.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def rotate ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Tuple [ Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]], float ]: \"\"\" Rotates an image for a face-on view (view from the top). Args: save: Saves the resultant image. inplace: Edits the image inplace. overriden_image: Overrides the image passes through the constructor. Returns: rotated_image: The rotated image. median_angle: The angly by which it is rotated. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img_edges = cv2 . Canny ( img , 100 , 100 , apertureSize = 3 ) lines = cv2 . HoughLinesP ( img_edges , rho = 1 , theta = np . pi / 180.0 , threshold = 160 , minLineLength = 100 , maxLineGap = 10 , ) angles = [] for [[ x1 , y1 , x2 , y2 ]] in lines : angle = math . degrees ( math . atan2 ( y2 - y1 , x2 - x1 )) angles . append ( angle ) median_angle = float ( np . median ( angles )) img = ndimage . rotate ( img , median_angle ) if save : cv2 . imwrite ( \"rotated.png\" , img ) return img , median_angle scan ( * , save = False , inplace = False , overriden_image = None ) Transforms an image/document view into B&W view (proper scanned colour scheme). Parameters: Name Type Description Default save Optional [ bool ] Saves the resultant image. False inplace Optional [ bool ] Edits the image inplace. False overriden_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None] Overrides the image passes through the constructor. None Returns: Name Type Description scanned_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]] The scanned image. Source code in ocred\\preprocessing.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def scan ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Transforms an image/document view into B&W view (proper scanned colour scheme). Args: save: Saves the resultant image. inplace: Edits the image inplace. overriden_image: Overrides the image passes through the constructor. Returns: scanned_image: The scanned image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) thr = threshold_local ( img , 11 , offset = 10 , method = \"gaussian\" ) img = ( img > thr ) . astype ( \"uint8\" ) * 255 if save : cv2 . imwrite ( \"scanned.png\" , img ) return img thicken_font ( * , save = False , inplace = False , iterations = 2 , overriden_image = None ) Thickens the ink of an image. Parameters: Name Type Description Default save Optional [ bool ] Saves the resultant image. False inplace Optional [ bool ] Edits the image inplace. False iterations Optional [ int ] Number of times the image is processed. 2 overriden_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None] Overrides the image passes through the constructor. None Returns: Name Type Description thickened_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]] The thickened image. Source code in ocred\\preprocessing.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def thicken_font ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , iterations : Optional [ int ] = 2 , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Thickens the ink of an image. Args: save: Saves the resultant image. inplace: Edits the image inplace. iterations: Number of times the image is processed. overriden_image: Overrides the image passes through the constructor. Returns: thickened_image: The thickened image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img = cv2 . bitwise_not ( img ) kernel : Union [ npt . NDArray [ np . int64 ]] = np . ones (( 2 , 2 ), np . uint8 ) img = cv2 . dilate ( img , kernel , iterations = iterations ) img = cv2 . bitwise_not ( img ) if save : cv2 . imwrite ( \"thick_font.png\" , img ) return img","title":"Reference"},{"location":"reference/#ocr-class","text":"Performs OCR on a given image, saves an image with boxes around the words, and converts the extracted text to an MP3 file. Add Tesseract OCR's installation location in PATH for functions using it to work. Parameters: Name Type Description Default preprocess bool Set True if the image is a real life photo of some large meaningful (page of a book). Usually set to False when OCRing using ocr_meaningful_text to preprocess the image. Set False if the image is a scanned photo (an e-book). It will not be pre-processed before OCRing. All the preprocessing is performed inplace to maintain efficiency. Use the Preprocessor class manually to have more control! required path str Path of the image to be used. required Examples: >>> import sys >>> sys . displayhook = lambda x : None >>> import ocred >>> ocr = ocred . OCR ( ... False , # is_scanned -> to preprocess the image ... \"./images/Page.png\" ... ) >>> ocr . ocr_meaningful_text ( save_output = True ) Source code in ocred\\ocr.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 class OCR : \"\"\" Performs OCR on a given image, saves an image with boxes around the words, and converts the extracted text to an MP3 file. Add Tesseract OCR's installation location in PATH for functions using it to work. Args: preprocess: Set True if the image is a real life photo of some large meaningful (page of a book). Usually set to False when OCRing using `ocr_meaningful_text` to preprocess the image. Set False if the image is a scanned photo (an e-book). It will not be pre-processed before OCRing. All the preprocessing is performed inplace to maintain efficiency. Use the `Preprocessor` class manually to have more control! path: Path of the image to be used. Examples: >>> import sys >>> sys.displayhook = lambda x: None >>> import ocred >>> ocr = ocred.OCR( ... False, # is_scanned -> to preprocess the image ... \"./images/Page.png\" ... ) >>> ocr.ocr_meaningful_text(save_output=True) \"\"\" def __init__ ( self , preprocess : bool , path : str ) -> None : self . path = path self . preprocess = preprocess if self . preprocess : preprocessed = Preprocessor ( self . path ) # scan the image and copy the scanned image scanned = preprocessed . scan ( inplace = True ) orig = scanned . copy () # remove noise noise_free = preprocessed . remove_noise ( inplace = True , overriden_image = scanned ) # thicken the ink to draw Hough lines better thickened = preprocessed . thicken_font ( inplace = True , overriden_image = noise_free ) # calculate the median angle of all the Hough lines _ , median_angle = preprocessed . rotate ( inplace = True , overriden_image = thickened ) # rotate the original scanned image rotated = ndimage . rotate ( orig , median_angle ) # remove noise again final_img = preprocessed . remove_noise ( inplace = True , overriden_image = rotated ) cv2 . imwrite ( \"preprocessed.png\" , final_img ) self . path = \"preprocessed.png\" def ocr_meaningful_text ( self , * , save_output : Optional [ bool ] = False ) -> str : \"\"\" Performs OCR on long meaningful text documents and saves the image with boxes around the words. For example - books, PDFs etc. Args: save_output: Saves the text to `output.txt` file. Returns: text: The extracted text. \"\"\" # reading the image img = cv2 . imread ( self . path ) # extracting the text self . oriented_text = pytesseract . image_to_string ( img , config = \"-l eng --oem 1\" ) self . text = self . oriented_text . replace ( \"- \\n \" , \"\" ) . replace ( \" \\n \" , \" \" ) # adding boxes around the words boxes = pytesseract . image_to_data ( img ) for z , box in enumerate ( boxes . splitlines ()): if z != 0 : box = box . split () # if the data has a word if len ( box ) == 12 : x , y = int ( box [ 6 ]), int ( box [ 7 ]) h , w = int ( box [ 8 ]), int ( box [ 9 ]) cv2 . rectangle ( img , ( x , y ), ( x + h , y + w ), ( 0 , 0 , 255 ), 1 ) cv2 . imwrite ( \"OCR.png\" , img ) if save_output : self . save_output () return self . text def ocr_sparse_text ( self , * , languages : Optional [ List [ str ]] = [ \"en\" , \"hi\" ], decoder : Optional [ str ] = \"greedy\" , save_output : Optional [ bool ] = False , ) -> str : \"\"\" Performs OCR on sparse text and saves the image with boxes around the words. This method can be used to OCR documents in which the characters don't form any proper/meaningful sentences, or if there are very less meaningful sentences, for example - bills, sign-boards etc. Args: languages: A list of languages that the signboard possible has. Note: Provide only the languages that are present in the image, adding additional languages misguides the model. decoder: If the document has a larger number of meaningful sentences then use \"beamsearch\". For most of the cases \"greedy\" works very well. save_output: Saves the text to `output.txt` file. Returns: text: The extracted text. \"\"\" self . text = \"\" # reading the image using open-cv and easyocr img = cv2 . imread ( self . path ) reader = easyocr . Reader ( languages ) # slow for the first time (also depends upon CPU/GPU) self . detailed_text = reader . readtext ( self . path , decoder = decoder , batch_size = 5 ) for text in self . detailed_text : # extracting the coordinates to highlight the text coords_lower = text [ 0 ][: 2 ] coords_upper = text [ 0 ][ 2 : 4 ] coords_lower . sort ( key = lambda x : x [ 0 ]) pt1 = [ int ( x ) for x in coords_upper [ - 1 ]] coords_lower . sort ( key = lambda x : x [ 0 ]) pt2 = [ int ( x ) for x in coords_lower [ - 1 ]] # highlighting the text cv2 . rectangle ( img , pt1 , pt2 , ( 0 , 0 , 255 ), 1 ) self . text = self . text + \" \" + text [ - 2 ] cv2 . imwrite ( \"OCR.png\" , img ) if save_output : self . save_output () return self . text def process_extracted_text_from_invoice ( self ) -> Dict [ str , Any ]: \"\"\" This method processes the extracted text from invoices, and returns some useful information. Returns: extracted_info: The extracted information. \"\"\" import nltk nltk . download ( \"punkt\" ) nltk . download ( \"wordnet\" ) nltk . download ( \"stopwords\" ) self . extracted_info = {} self . text_list = self . text . split ( \" \" ) # find date date_re = re . compile ( r \"^([1-9]|0[1-9]|1[0-9]|2[0-9]|3[0-1])(\\.|-|\\/)([1-9]|0[1-9]|1[0-2])(\\.|-|\\/)([0-9][0-9]|19[0-9][0-9]|20[0-9][0-9])$\" , ) date = list ( filter ( date_re . match , self . text_list )) # find phone number phone_number_re = re . compile ( r \"((\\+*)((0[ -]*)*|((91 )*))((\\d {12} )+|(\\d {10} )+))|\\d {5} ([- ]*)\\d {6} \" , ) phone_number = list ( filter ( phone_number_re . match , self . text_list )) # find place place = self . detailed_text [ 0 ][ - 2 ] # remove puntuations and redundant words tokenizer = nltk . RegexpTokenizer ( r \"\\w+\" ) removed_punctuation = tokenizer . tokenize ( self . text ) stop_words = set ( nltk . corpus . stopwords . words ( \"english\" )) post_processed_word_list = [ w for w in removed_punctuation if w not in stop_words ] # find order number order_number : Union [ str , int ] = \"\" for i in range ( len ( post_processed_word_list )): if post_processed_word_list [ i ] . lower () == \"order\" : try : order_number = int ( post_processed_word_list [ i + 1 ]) except Exception : order_number = post_processed_word_list [ i + 2 ] break # find total price price : Union [ List [ Any ], str ] = \"\" # try finding a number with Rs, INR, \u20b9 or \u0930\u0947 in front of it or Rs, INR at the end # of it try : price = re . findall ( r \"(?:Rs\\.?|INR|\u20b9\\.?|\u0930\u0947\\.?)\\s*(\\d+(?:[.,]\\d+)*)|(\\d+(?:[.,]\\d+)*)\\s*(?:Rs\\.?|INR)\" , self . text , ) price = list ( map ( float , price )) price = max ( price ) # try finding numbers with \"grand total\" or \"total\" written in front of them except ValueError : lowered_list = [ x . lower () for x in post_processed_word_list ] if \"grand\" in lowered_list : indices = [ i for i , x in enumerate ( lowered_list ) if x == \"grand\" ] i = indices [ - 1 ] price = post_processed_word_list [ i + 2 ] elif \"total\" in lowered_list : indices = [ i for i , x in enumerate ( lowered_list ) if x == \"total\" ] i = indices [ - 1 ] price = post_processed_word_list [ i + 1 ] self . extracted_info . update ( { \"price\" : price , \"date\" : date , \"place\" : place , \"order_number\" : order_number , \"phone_number\" : phone_number , \"post_processed_word_list\" : post_processed_word_list , } ) return self . extracted_info def save_output ( self ) -> None : \"\"\"Saves the extracted text in the `output.txt` file.\"\"\" f = open ( \"output.txt\" , \"w\" , encoding = \"utf-8\" ) f . write ( self . text ) f . close () def text_to_speech ( self , * , lang : Optional [ str ] = \"en\" ) -> None : \"\"\" DANGER: Deprecated since version v0.2.0. Instead, use gTTS manually. Converts the extracted text to speech and save it as an MP3 file. Args: lang: Language of the processed text. \"\"\" raise DeprecationWarning ( \"text_to_speech is deprecated and was removed in v0.2.0; use gTTS manually\" , )","title":"OCR class"},{"location":"reference/#ocred.ocr.OCR.ocr_meaningful_text","text":"Performs OCR on long meaningful text documents and saves the image with boxes around the words. For example - books, PDFs etc. Parameters: Name Type Description Default save_output Optional [ bool ] Saves the text to output.txt file. False Returns: Name Type Description text str The extracted text. Source code in ocred\\ocr.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def ocr_meaningful_text ( self , * , save_output : Optional [ bool ] = False ) -> str : \"\"\" Performs OCR on long meaningful text documents and saves the image with boxes around the words. For example - books, PDFs etc. Args: save_output: Saves the text to `output.txt` file. Returns: text: The extracted text. \"\"\" # reading the image img = cv2 . imread ( self . path ) # extracting the text self . oriented_text = pytesseract . image_to_string ( img , config = \"-l eng --oem 1\" ) self . text = self . oriented_text . replace ( \"- \\n \" , \"\" ) . replace ( \" \\n \" , \" \" ) # adding boxes around the words boxes = pytesseract . image_to_data ( img ) for z , box in enumerate ( boxes . splitlines ()): if z != 0 : box = box . split () # if the data has a word if len ( box ) == 12 : x , y = int ( box [ 6 ]), int ( box [ 7 ]) h , w = int ( box [ 8 ]), int ( box [ 9 ]) cv2 . rectangle ( img , ( x , y ), ( x + h , y + w ), ( 0 , 0 , 255 ), 1 ) cv2 . imwrite ( \"OCR.png\" , img ) if save_output : self . save_output () return self . text","title":"ocr_meaningful_text()"},{"location":"reference/#ocred.ocr.OCR.ocr_sparse_text","text":"Performs OCR on sparse text and saves the image with boxes around the words. This method can be used to OCR documents in which the characters don't form any proper/meaningful sentences, or if there are very less meaningful sentences, for example - bills, sign-boards etc. Parameters: Name Type Description Default languages Optional [ List [ str ]] A list of languages that the signboard possible has. Note: Provide only the languages that are present in the image, adding additional languages misguides the model. ['en', 'hi'] decoder Optional [ str ] If the document has a larger number of meaningful sentences then use \"beamsearch\". For most of the cases \"greedy\" works very well. 'greedy' save_output Optional [ bool ] Saves the text to output.txt file. False Returns: Name Type Description text str The extracted text. Source code in ocred\\ocr.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def ocr_sparse_text ( self , * , languages : Optional [ List [ str ]] = [ \"en\" , \"hi\" ], decoder : Optional [ str ] = \"greedy\" , save_output : Optional [ bool ] = False , ) -> str : \"\"\" Performs OCR on sparse text and saves the image with boxes around the words. This method can be used to OCR documents in which the characters don't form any proper/meaningful sentences, or if there are very less meaningful sentences, for example - bills, sign-boards etc. Args: languages: A list of languages that the signboard possible has. Note: Provide only the languages that are present in the image, adding additional languages misguides the model. decoder: If the document has a larger number of meaningful sentences then use \"beamsearch\". For most of the cases \"greedy\" works very well. save_output: Saves the text to `output.txt` file. Returns: text: The extracted text. \"\"\" self . text = \"\" # reading the image using open-cv and easyocr img = cv2 . imread ( self . path ) reader = easyocr . Reader ( languages ) # slow for the first time (also depends upon CPU/GPU) self . detailed_text = reader . readtext ( self . path , decoder = decoder , batch_size = 5 ) for text in self . detailed_text : # extracting the coordinates to highlight the text coords_lower = text [ 0 ][: 2 ] coords_upper = text [ 0 ][ 2 : 4 ] coords_lower . sort ( key = lambda x : x [ 0 ]) pt1 = [ int ( x ) for x in coords_upper [ - 1 ]] coords_lower . sort ( key = lambda x : x [ 0 ]) pt2 = [ int ( x ) for x in coords_lower [ - 1 ]] # highlighting the text cv2 . rectangle ( img , pt1 , pt2 , ( 0 , 0 , 255 ), 1 ) self . text = self . text + \" \" + text [ - 2 ] cv2 . imwrite ( \"OCR.png\" , img ) if save_output : self . save_output () return self . text","title":"ocr_sparse_text()"},{"location":"reference/#ocred.ocr.OCR.process_extracted_text_from_invoice","text":"This method processes the extracted text from invoices, and returns some useful information. Returns: Name Type Description extracted_info Dict [ str , Any ] The extracted information. Source code in ocred\\ocr.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def process_extracted_text_from_invoice ( self ) -> Dict [ str , Any ]: \"\"\" This method processes the extracted text from invoices, and returns some useful information. Returns: extracted_info: The extracted information. \"\"\" import nltk nltk . download ( \"punkt\" ) nltk . download ( \"wordnet\" ) nltk . download ( \"stopwords\" ) self . extracted_info = {} self . text_list = self . text . split ( \" \" ) # find date date_re = re . compile ( r \"^([1-9]|0[1-9]|1[0-9]|2[0-9]|3[0-1])(\\.|-|\\/)([1-9]|0[1-9]|1[0-2])(\\.|-|\\/)([0-9][0-9]|19[0-9][0-9]|20[0-9][0-9])$\" , ) date = list ( filter ( date_re . match , self . text_list )) # find phone number phone_number_re = re . compile ( r \"((\\+*)((0[ -]*)*|((91 )*))((\\d {12} )+|(\\d {10} )+))|\\d {5} ([- ]*)\\d {6} \" , ) phone_number = list ( filter ( phone_number_re . match , self . text_list )) # find place place = self . detailed_text [ 0 ][ - 2 ] # remove puntuations and redundant words tokenizer = nltk . RegexpTokenizer ( r \"\\w+\" ) removed_punctuation = tokenizer . tokenize ( self . text ) stop_words = set ( nltk . corpus . stopwords . words ( \"english\" )) post_processed_word_list = [ w for w in removed_punctuation if w not in stop_words ] # find order number order_number : Union [ str , int ] = \"\" for i in range ( len ( post_processed_word_list )): if post_processed_word_list [ i ] . lower () == \"order\" : try : order_number = int ( post_processed_word_list [ i + 1 ]) except Exception : order_number = post_processed_word_list [ i + 2 ] break # find total price price : Union [ List [ Any ], str ] = \"\" # try finding a number with Rs, INR, \u20b9 or \u0930\u0947 in front of it or Rs, INR at the end # of it try : price = re . findall ( r \"(?:Rs\\.?|INR|\u20b9\\.?|\u0930\u0947\\.?)\\s*(\\d+(?:[.,]\\d+)*)|(\\d+(?:[.,]\\d+)*)\\s*(?:Rs\\.?|INR)\" , self . text , ) price = list ( map ( float , price )) price = max ( price ) # try finding numbers with \"grand total\" or \"total\" written in front of them except ValueError : lowered_list = [ x . lower () for x in post_processed_word_list ] if \"grand\" in lowered_list : indices = [ i for i , x in enumerate ( lowered_list ) if x == \"grand\" ] i = indices [ - 1 ] price = post_processed_word_list [ i + 2 ] elif \"total\" in lowered_list : indices = [ i for i , x in enumerate ( lowered_list ) if x == \"total\" ] i = indices [ - 1 ] price = post_processed_word_list [ i + 1 ] self . extracted_info . update ( { \"price\" : price , \"date\" : date , \"place\" : place , \"order_number\" : order_number , \"phone_number\" : phone_number , \"post_processed_word_list\" : post_processed_word_list , } ) return self . extracted_info","title":"process_extracted_text_from_invoice()"},{"location":"reference/#ocred.ocr.OCR.save_output","text":"Saves the extracted text in the output.txt file. Source code in ocred\\ocr.py 270 271 272 273 274 def save_output ( self ) -> None : \"\"\"Saves the extracted text in the `output.txt` file.\"\"\" f = open ( \"output.txt\" , \"w\" , encoding = \"utf-8\" ) f . write ( self . text ) f . close ()","title":"save_output()"},{"location":"reference/#ocred.ocr.OCR.text_to_speech","text":"Danger Deprecated since version v0.2.0. Instead, use gTTS manually. Converts the extracted text to speech and save it as an MP3 file. Parameters: Name Type Description Default lang Optional [ str ] Language of the processed text. 'en' Source code in ocred\\ocr.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def text_to_speech ( self , * , lang : Optional [ str ] = \"en\" ) -> None : \"\"\" DANGER: Deprecated since version v0.2.0. Instead, use gTTS manually. Converts the extracted text to speech and save it as an MP3 file. Args: lang: Language of the processed text. \"\"\" raise DeprecationWarning ( \"text_to_speech is deprecated and was removed in v0.2.0; use gTTS manually\" , )","title":"text_to_speech()"},{"location":"reference/#preprocessor-class","text":"Preprocesses an image and makes it ready for OCR. Parameters: Name Type Description Default image Union [ str , Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]] Path of the image or a numpy array. required Examples: >>> import cv2 >>> from scipy import ndimage >>> from ocred import Preprocessor >>> # scan the image and copy the scanned image >>> preprocessed = Preprocessor ( \"images/CosmosTwo.jpg\" ) >>> # scan the image and copy the scanned image >>> scanned = preprocessed . scan ( inplace = True ) >>> orig = scanned . copy () >>> # remove noise ... noise_free = preprocessed . remove_noise ( ... inplace = True , overriden_image = scanned ... ) >>> # thicken the ink to draw Hough lines better >>> thickened = preprocessed . thicken_font ( ... inplace = True , overriden_image = noise_free ... ) >>> # calculate the median angle of all the Hough lines >>> _ , median_angle = preprocessed . rotate ( ... inplace = True , overriden_image = thickened ... ) >>> # rotate the original scanned image >>> rotated = ndimage . rotate ( orig , median_angle ) >>> # remove noise again >>> final_img = preprocessed . remove_noise ( inplace = True , overriden_image = rotated ) >>> cv2 . imwrite ( \"preprocessed.png\" , final_img ) True Source code in ocred\\preprocessing.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class Preprocessor : \"\"\" Preprocesses an image and makes it ready for OCR. Args: image: Path of the image or a numpy array. Examples: >>> import cv2 >>> from scipy import ndimage >>> from ocred import Preprocessor >>> # scan the image and copy the scanned image >>> preprocessed = Preprocessor(\"images/CosmosTwo.jpg\") >>> # scan the image and copy the scanned image >>> scanned = preprocessed.scan(inplace=True) >>> orig = scanned.copy() >>> # remove noise ... noise_free = preprocessed.remove_noise( ... inplace=True, overriden_image=scanned ... ) >>> # thicken the ink to draw Hough lines better >>> thickened = preprocessed.thicken_font( ... inplace=True, overriden_image=noise_free ... ) >>> # calculate the median angle of all the Hough lines >>> _, median_angle = preprocessed.rotate( ... inplace=True, overriden_image=thickened ... ) >>> # rotate the original scanned image >>> rotated = ndimage.rotate(orig, median_angle) >>> # remove noise again >>> final_img = preprocessed.remove_noise(inplace=True, overriden_image=rotated) >>> cv2.imwrite(\"preprocessed.png\", final_img) True \"\"\" def __init__ ( self , image : Union [ str , Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]], ) -> None : if isinstance ( image , str ): self . img = cv2 . imread ( image ) else : self . img = image def remove_noise ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , iterations : Optional [ int ] = 1 , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Removes noise from an image. Args: save: Saves the resultant image. inplace: Edits the image inplace. iterations: Number of times the image is processed. overriden_image: Overrides the image passes through the constructor. Returns: noise_free_image: The noise free image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image kernel : Union [ npt . NDArray [ np . int64 ]] = np . ones (( 1 , 1 ), np . uint8 ) img = cv2 . dilate ( img , kernel , iterations = iterations ) kernel = np . ones (( 1 , 1 ), np . uint8 ) img = cv2 . erode ( img , kernel , iterations = iterations ) img = cv2 . morphologyEx ( img , cv2 . MORPH_CLOSE , kernel ) img = cv2 . medianBlur ( img , 3 ) if save : cv2 . imwrite ( \"noise_free.png\" , img ) return self . img def thicken_font ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , iterations : Optional [ int ] = 2 , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Thickens the ink of an image. Args: save: Saves the resultant image. inplace: Edits the image inplace. iterations: Number of times the image is processed. overriden_image: Overrides the image passes through the constructor. Returns: thickened_image: The thickened image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img = cv2 . bitwise_not ( img ) kernel : Union [ npt . NDArray [ np . int64 ]] = np . ones (( 2 , 2 ), np . uint8 ) img = cv2 . dilate ( img , kernel , iterations = iterations ) img = cv2 . bitwise_not ( img ) if save : cv2 . imwrite ( \"thick_font.png\" , img ) return img def scan ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Transforms an image/document view into B&W view (proper scanned colour scheme). Args: save: Saves the resultant image. inplace: Edits the image inplace. overriden_image: Overrides the image passes through the constructor. Returns: scanned_image: The scanned image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) thr = threshold_local ( img , 11 , offset = 10 , method = \"gaussian\" ) img = ( img > thr ) . astype ( \"uint8\" ) * 255 if save : cv2 . imwrite ( \"scanned.png\" , img ) return img def rotate ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Tuple [ Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]], float ]: \"\"\" Rotates an image for a face-on view (view from the top). Args: save: Saves the resultant image. inplace: Edits the image inplace. overriden_image: Overrides the image passes through the constructor. Returns: rotated_image: The rotated image. median_angle: The angly by which it is rotated. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img_edges = cv2 . Canny ( img , 100 , 100 , apertureSize = 3 ) lines = cv2 . HoughLinesP ( img_edges , rho = 1 , theta = np . pi / 180.0 , threshold = 160 , minLineLength = 100 , maxLineGap = 10 , ) angles = [] for [[ x1 , y1 , x2 , y2 ]] in lines : angle = math . degrees ( math . atan2 ( y2 - y1 , x2 - x1 )) angles . append ( angle ) median_angle = float ( np . median ( angles )) img = ndimage . rotate ( img , median_angle ) if save : cv2 . imwrite ( \"rotated.png\" , img ) return img , median_angle","title":"Preprocessor class"},{"location":"reference/#ocred.preprocessing.Preprocessor.remove_noise","text":"Removes noise from an image. Parameters: Name Type Description Default save Optional [ bool ] Saves the resultant image. False inplace Optional [ bool ] Edits the image inplace. False iterations Optional [ int ] Number of times the image is processed. 1 overriden_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None] Overrides the image passes through the constructor. None Returns: Name Type Description noise_free_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]] The noise free image. Source code in ocred\\preprocessing.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def remove_noise ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , iterations : Optional [ int ] = 1 , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Removes noise from an image. Args: save: Saves the resultant image. inplace: Edits the image inplace. iterations: Number of times the image is processed. overriden_image: Overrides the image passes through the constructor. Returns: noise_free_image: The noise free image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image kernel : Union [ npt . NDArray [ np . int64 ]] = np . ones (( 1 , 1 ), np . uint8 ) img = cv2 . dilate ( img , kernel , iterations = iterations ) kernel = np . ones (( 1 , 1 ), np . uint8 ) img = cv2 . erode ( img , kernel , iterations = iterations ) img = cv2 . morphologyEx ( img , cv2 . MORPH_CLOSE , kernel ) img = cv2 . medianBlur ( img , 3 ) if save : cv2 . imwrite ( \"noise_free.png\" , img ) return self . img","title":"remove_noise()"},{"location":"reference/#ocred.preprocessing.Preprocessor.rotate","text":"Rotates an image for a face-on view (view from the top). Parameters: Name Type Description Default save Optional [ bool ] Saves the resultant image. False inplace Optional [ bool ] Edits the image inplace. False overriden_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None] Overrides the image passes through the constructor. None Returns: Name Type Description rotated_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]] The rotated image. median_angle float The angly by which it is rotated. Source code in ocred\\preprocessing.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def rotate ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Tuple [ Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]], float ]: \"\"\" Rotates an image for a face-on view (view from the top). Args: save: Saves the resultant image. inplace: Edits the image inplace. overriden_image: Overrides the image passes through the constructor. Returns: rotated_image: The rotated image. median_angle: The angly by which it is rotated. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img_edges = cv2 . Canny ( img , 100 , 100 , apertureSize = 3 ) lines = cv2 . HoughLinesP ( img_edges , rho = 1 , theta = np . pi / 180.0 , threshold = 160 , minLineLength = 100 , maxLineGap = 10 , ) angles = [] for [[ x1 , y1 , x2 , y2 ]] in lines : angle = math . degrees ( math . atan2 ( y2 - y1 , x2 - x1 )) angles . append ( angle ) median_angle = float ( np . median ( angles )) img = ndimage . rotate ( img , median_angle ) if save : cv2 . imwrite ( \"rotated.png\" , img ) return img , median_angle","title":"rotate()"},{"location":"reference/#ocred.preprocessing.Preprocessor.scan","text":"Transforms an image/document view into B&W view (proper scanned colour scheme). Parameters: Name Type Description Default save Optional [ bool ] Saves the resultant image. False inplace Optional [ bool ] Edits the image inplace. False overriden_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None] Overrides the image passes through the constructor. None Returns: Name Type Description scanned_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]] The scanned image. Source code in ocred\\preprocessing.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def scan ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Transforms an image/document view into B&W view (proper scanned colour scheme). Args: save: Saves the resultant image. inplace: Edits the image inplace. overriden_image: Overrides the image passes through the constructor. Returns: scanned_image: The scanned image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) thr = threshold_local ( img , 11 , offset = 10 , method = \"gaussian\" ) img = ( img > thr ) . astype ( \"uint8\" ) * 255 if save : cv2 . imwrite ( \"scanned.png\" , img ) return img","title":"scan()"},{"location":"reference/#ocred.preprocessing.Preprocessor.thicken_font","text":"Thickens the ink of an image. Parameters: Name Type Description Default save Optional [ bool ] Saves the resultant image. False inplace Optional [ bool ] Edits the image inplace. False iterations Optional [ int ] Number of times the image is processed. 2 overriden_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None] Overrides the image passes through the constructor. None Returns: Name Type Description thickened_image Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]] The thickened image. Source code in ocred\\preprocessing.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def thicken_font ( self , * , save : Optional [ bool ] = False , inplace : Optional [ bool ] = False , iterations : Optional [ int ] = 2 , overriden_image : Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ], None ] = None ) -> Union [ npt . NDArray [ np . int64 ], npt . NDArray [ np . float64 ]]: \"\"\" Thickens the ink of an image. Args: save: Saves the resultant image. inplace: Edits the image inplace. iterations: Number of times the image is processed. overriden_image: Overrides the image passes through the constructor. Returns: thickened_image: The thickened image. \"\"\" if not inplace : img = self . img . copy () if overriden_image is None else overriden_image . copy () else : img = self . img if overriden_image is None else overriden_image img = cv2 . bitwise_not ( img ) kernel : Union [ npt . NDArray [ np . int64 ]] = np . ones (( 2 , 2 ), np . uint8 ) img = cv2 . dilate ( img , kernel , iterations = iterations ) img = cv2 . bitwise_not ( img ) if save : cv2 . imwrite ( \"thick_font.png\" , img ) return img","title":"thicken_font()"}]}